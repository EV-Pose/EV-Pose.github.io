<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EV-Pose">
  <meta name="keywords" content="Event camera">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>[MobiCom 2025] Enabling High-Frequency Cross-Modality Visual Positioning Service for Drones</title>

  <script type="text/javascript" async
  src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet"> -->

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enabling High-Frequency Cross-Modality Visual Positioning Service for Drones</h1>
          <div class="is-size-5 publication-authors">
            <h1 class="title is-4 publication-title", style="color:#6e6e6e;">ACM MobiCom 2025 Submissions #488</h1>
            <span class="author-block">
              Anonymous authors</a></span>
            
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ev-pose/ev-pose.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=" "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align:center">
      <h2 class="title is-3">Demo video</h2>

      <h2 class="content has-text-justified">
        EV-Pose estimates drone 6-DoF pose by redesigning drone-oriented VPS with event cameras. 
        Compared to conventional VPS systems, EV-Pose enables rapid and high-frequency drone pose tracking, ensuring precise flight control.

      </h2>

      <video id="teaser" width="640" height="480" autoplay muted loop playsinline height="100%">
        <source src="./static/images/demo.mp4" type="video/mp4">
      </video>
      <!-- <img src="./static/images/demo.png" alt="Image 1" width="100%"> -->

      <h2 class="subtitle has-text-centered">
        <span class="dnerf">EV-Pose redesigns drone-oriented VPS with event cameras. </span> 
      </h2>
    </div>
  </div>
</section>

<hr>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          After years of growth, the drone-driven low-altitude economy is transforming logistics.
          At its core, real-time 6-DoF drone pose tracking enables precise flight control.  
          With widely accessible of urban 3D maps, Visual Positioning Service (VPS), a mobile pose estimation system, has been adapted to improve drone pose tracking in GPS-denied environments.
          However, deploying current VPS on drones faces limitations in both estimation accuracy and efficiency.
          In this work, we redesign drone-oriented VPS with event camera and introduce EV-Pose to enable accurate, low-latency 6-DoF drone pose tracking.
          EV-Pose introduces a spatio-temporal feature-instructed pose estimation module to extract temporal distance field for pose estimation with 3D point map matching; 
          and a motion-aware hierarchical fusion and optimization scheme to enhance above estimation in accuracy and efficiency, by utilizing drone motion in early stage of event filtering and later stage of pose optimization.
          Evaluation results show that EV-Pose reduces translation and rotation errors by up to 72.63% and 90.72%, satisfying frequency compatibility with flight controller.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Event Camera Preliminary</h2>
        <div class="content has-text-justified">
        <p>
          <!-- Event cameras are bio-inspired sensors that capture data asynchronously and independently at the pixel level.  -->
          Event cameras are bio-inspired sensors that differ from traditional frame cameras. 
          Specifically, frame cameras capture synchronous images with a global shutter at fixed time intervals, while output of an event camera is event streams with $ms$-level resolution, as shown in Fig. (a).
          Each pixel in an event camera independently responds to changes in brightness asynchronously.
          Each event \( e = (\boldsymbol{x}, i, p) \) represents a pixel at location \(\boldsymbol{x} = (u, v)\) has undergone a predefined magnitude change in brightness at a time \( i \), as shown in Fig. (b). 
          \( p \) is polarity of intensity change, which is 'ON' for brighter and 'OFF' for darker.
        </p>
          <!-- <p>
          When a pixel detects a sufficient change in light intensity, it generates an event, denoted as \( e_k = \left(\mathbf{x}_k, t_k, p_k\right) \). 
          This event encodes the pixel’s position \(\mathbf{x}_k\), the exact time of activation \(t_k\), and the polarity \(p_k\) (where \(+1\) represents a brightening change and \(-1\) a darkening change) of the intensity shift.
        </p> -->
        </div>
        <img src="./static/images/event.png" alt="Image 1" width="100%">
      </div>
    </div>

  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">RGB camera-based VPS & event camera-enhanced VPS</h2>
        <div class="content has-text-justified">
        <p>
          <!-- Event cameras are bio-inspired sensors that capture data asynchronously and independently at the pixel level.  -->
          (a) Current VPS uses an RGB camera, an IMU, and 3D point clouds for pose estimation. <br>
          (b) EV-Pose leverages event cameras for accurate and low-latency 6-DoF drone pose tracking.
        </p>
          <!-- <p>
          When a pixel detects a sufficient change in light intensity, it generates an event, denoted as \( e_k = \left(\mathbf{x}_k, t_k, p_k\right) \). 
          This event encodes the pixel’s position \(\mathbf{x}_k\), the exact time of activation \(t_k\), and the polarity \(p_k\) (where \(+1\) represents a brightening change and \(-1\) a darkening change) of the intensity shift.
        </p> -->
        </div>
        <img src="./static/images/comparation.png" alt="Image 1" width="70%">
      </div>
    </div>

  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">System Overview</h2>
        <div class="content has-text-justified">
        <p>
          From a top-level perspective, we design EV-Pose, an event-based 6-DoF pose tracking system for drones that redesign current VPS with event camera. 
          EV-Pose leverages prior 3D point maps and temporal consistency between event camera and IMU to achieve accurate, low-latency drone 6-DoF pose tracking.
        </p>
        <p>
          <b>(i) Feature-instructed Pose Estimation (STPE) module </b> (§ 4).
          This module first introduces the concept of a separated polarity time-surface, a novel spatio-temporal representation for event streams (§4.1). 
          Subsequently, it leverages the temporal relationships among events encoded in the time-surface to generate a distance field, which is then used as a feature representation for the event stream (§4.2).
          Finally, the 2D Event-3D Point Map Matching module models the drone pose estimation problem, which aligns the event stream's distance field feature with the 3D point map, thus facilitating absolute pose estimation of the drone (§4.3).
        
        </p>
        <p>
          <b>(ii) Motion-aware Hierarchical Fusion and Optimization (MHFO) scheme </b> (§5).
          This scheme first introduces motion-optical flow-instructed event filtering (§5.1), which combines drone motion information with structural data from the 3D point map to predict event polarity and perform fine-grained event filtering. 
          This approach fuses event camera with IMU at the early stage of raw data processing, significantly reducing the number of events during matching. 
          By utilizing proprioceptive motion tracking to infer the drone's relative motion (§5.2), this scheme introduces a joint fusion and optimization module (§5.3), based on a factor graph, which treats relative motion as proprioceptive measurements and integrates them with the exteroceptive measurements from the STPF module. 
          This fusion, performed at the later stage of pose estimation, yields optimized 6-DoF poses.

        </p>

        <p>
        <b>Relationship between STPE and MHFO.</b> 
        STPE extracts a temporal distance field feature from the event stream and aligns it with a prior 3D point map to facilitate map-based drone pose estimation.
        To further enhance the efficiency and accuracy of this estimation, EV-Pose incorporates MHFO, which leverages drone motion information for early-stage event filtering—reducing the number of events involved in matching—and later-stage pose optimization, recovering the scale and producing a 6-DoF trajectory with minimal drift.
        </p>
        <img src="./static/images/overview.png" alt="Image 1" width="100%">
        </div>
      </div>
    </div>

  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Implementation and Experiments Setup</h2>
        <div class="content has-text-justified">
          As illustrated in figure, EV-Pose is implemented using a 450 mm-wide drone equipped with <br>
          <b>(i)</b> a Prophesee EVK4 HD event camera with 1280 x 720 resolution; <br>
          <b>(ii)</b>  a D435i Depth camera for frame image capture; <br>
          <b>(iii)</b> a Pixhawk 4 flight controller for drone control and IMU measurements. <br>
          EV-Pose runs on an Intel NUC with a Core i7 CPU, 16GB RAM, and Ubuntu 20.04.
          Indoor and outdoor environment mapping is completed in advance using the Livos MID-360 LiDAR and FAST-LIO2 algorithm.
          All algorithms of EV-Pose are implemented in C++.
          Several implementation techniques are employed to enhance the practicality of the system and push the limits of tracking accuracy and efficiency.         
        </div>
        <img src="./static/images/implement.png" alt="Image 1" width="100%">
        <br><br><br>
        <div class="content has-text-justified">
          We conduct field studies of EV-Pose both indoors and outdoors to evaluate its pose tracking performance, as shown in figure.
          The environment is pre-mapped, and the drone operates in the 3D point-mapped environment for pose estimation. 
          In indoor settings, the drone's pose ground truth is obtained using a CHINGMU Motion Capture system with 16 MC1300 infrared cameras operating at 210 FPS. 
          For outdoor settings, we deployed a private RTK station using a Hi-Target D8 to provide accurate pose ground truth.
        </div>
        <img src="./static/images/setup.png" alt="Image 1" width="100%">

      </div>
    </div>

  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative results</h2>
        <h4 class="title is-4">Overall Performances on Field Study</h4>
        <img src="./static/images/overallerror.png" alt="Image 1" width="100%">
        <br><br>
        <h4 class="title is-4">Latency on Field Study</h4>
        <img src="./static/images/latency.png" alt="Image 1" width="70%">
        <br><br>
        <h4 class="title is-4">Overall Performances on Public Datasets</h4>
        <img src="./static/images/dataset.png" alt="Image 1" width="100%">
        <br><br>
        <h4 class="title is-4">Robustness Evaluation</h4>
        <img src="./static/images/robustness.png" alt="Image 1" width="100%">
        <br><br>
        <h4 class="title is-4">Ablation Study</h4>
        <img src="./static/images/ablation.png" alt="Image 1" width="100%">
        <br><br>
        <h4 class="title is-4">System Overload</h4>
        <img src="./static/images/overload.png" alt="Image 1" width="100%">

      </div>
    </div>

  </div>
</section>

<hr>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
